{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'D:\\Evans microbial community\\data_for_pipeline\\metadata_full_MMPRNT_G5.LC_LUX_trunc_rar_016017_v2.txt'\n",
    "#label = 'D:\\Evans microbial community\\data_for_pipeline\\ELSA_module_016017_LUX_OTU_sum_MMPRNT_G5_LC_LUX_016017.txt'\n",
    "label = 'D:\\Evans microbial community\\data_for_pipeline\\Rarefied_diversity_MMPRNT_G5_LC_LUX_016017.txt'\n",
    "site = 'LUX'\n",
    "site_not_used = 'LC'\n",
    "cv_num = 10\n",
    "test_size = 0.1 ### what the proportion of your data you want to hold out as test set, \n",
    "                ### which will never be seen when you train the model\n",
    "feature_2_onehotencoding = ['FertStatus','thermal_two_year','thermal_2019','thermal_2018']\n",
    "save_model_name = 'Multioutput_model_network_modules.sav'\n",
    "save_result_name = 'Result_Multioutput_network_modules.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(file, sep='\\t', index_col = 0, header = 0)\n",
    "if 'ELSA' in label:\n",
    "    df_lable = pd.read_csv(label, sep='\\t', index_col = 42, header = 0)\n",
    "if 'Rarefied' in label:\n",
    "    df_lable = pd.read_csv(label, sep='\\t', index_col = 0, header = 0)\n",
    "ML_matrix = pd.concat([df_lable,df], axis=1, sort = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_target_site = ML_matrix.loc[ML_matrix['siteID']==site]\n",
    "df_target_site = df_target_site.drop([\"collectionDate\",\"siteID\",\\\n",
    "                                             \"UTM_Lat_Cord\",\"UTM_Lon_Cord\"],axis=1)\n",
    "\n",
    "df_other_site = ML_matrix.loc[ML_matrix['siteID']==site_not_used]\n",
    "df_other_site = df_other_site.drop([\"collectionDate\",\"siteID\",\\\n",
    "                                             \"UTM_Lat_Cord\",\"UTM_Lon_Cord\"],axis=1)\n",
    "\n",
    "df_target_site[\"thermal_two_year\"].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data to traning and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_set, test_set = train_test_split(df_target_site, \\\n",
    "                                       test_size=test_size, random_state=42)\n",
    "X_train = train_set.drop(df_lable.columns, axis=1) \n",
    "X_test = test_set.drop(df_lable.columns, axis=1)\n",
    "X_on_test_site = df_other_site.drop(df_lable.columns, axis=1)\n",
    "\n",
    "y_train = train_set[df_lable.columns]\n",
    "y_test = test_set[df_lable.columns]\n",
    "y_on_test_site = df_other_site[df_lable.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OneHotEncoding, handle with NaN data (keep them as NaN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit on training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "def OneHotEncoder_fit_transform(df,feature_2_onehotencoding):\n",
    "    new_columns = []\n",
    "    Onehot = {}\n",
    "    col_2_1hot = df.loc[:,feature_2_onehotencoding]\n",
    "    for col in feature_2_onehotencoding:\n",
    "        Onehot[col] = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "        c_1hot_use = pd.DataFrame(col_2_1hot.loc[col_2_1hot.loc[:,col].notna(),col])\n",
    "        c_1hot_na = pd.DataFrame(col_2_1hot.loc[col_2_1hot.loc[:,col].isna(),col])\n",
    "        if c_1hot_na.shape[0] == 0:\n",
    "            c_1hot_encoded = pd.DataFrame(Onehot[col].fit_transform(c_1hot_use))\n",
    "            c_1hot_encoded.columns = [col + '_' + '%s'%sub for sub in Onehot[col].categories_[0]]\n",
    "            c_1hot_encoded.index = col_2_1hot.index\n",
    "            for columns in c_1hot_encoded.columns:\n",
    "                new_columns.append(columns)\n",
    "        if c_1hot_na.shape[0] != 0:\n",
    "            c_1hot_encoded = pd.DataFrame(Onehot[col].fit_transform(c_1hot_use))\n",
    "            c_1hot_encoded.columns = [col + '_' + '%s'%sub for sub in Onehot[col].categories_[0]]\n",
    "            c_1hot_encoded.index = c_1hot_use.index\n",
    "            for columns in c_1hot_encoded.columns:\n",
    "                c_1hot_na[columns] = np.nan\n",
    "                new_columns.append(columns)\n",
    "            c_1hot_na = c_1hot_na.drop(col,axis=1)\n",
    "            c_1hot_encoded = pd.concat([c_1hot_encoded,c_1hot_na],axis=0)\n",
    "        df = pd.concat([df,c_1hot_encoded],axis=1)\n",
    "    return(df,Onehot,new_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### transform on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OneHotEncoder_transform(df,feature_2_onehotencoding,Onehot):\n",
    "    col_2_1hot = df.loc[:,feature_2_onehotencoding]\n",
    "    for col in feature_2_onehotencoding:\n",
    "        c_1hot_use = pd.DataFrame(col_2_1hot.loc[col_2_1hot.loc[:,col].notna(),col])\n",
    "        c_1hot_na = pd.DataFrame(col_2_1hot.loc[col_2_1hot.loc[:,col].isna(),col])\n",
    "        if c_1hot_na.shape[0] == 0:\n",
    "            c_1hot_encoded = pd.DataFrame(Onehot[col].transform(c_1hot_use))\n",
    "            c_1hot_encoded.columns = [col + '_' + '%s'%sub for sub in Onehot[col].categories_[0]]\n",
    "            c_1hot_encoded.index = col_2_1hot.index\n",
    "            df = pd.concat([df,c_1hot_encoded],axis=1)\n",
    "        if c_1hot_na.shape[0] != 0 and c_1hot_use.shape[0] != 0:\n",
    "            c_1hot_encoded = pd.DataFrame(Onehot[col].transform(c_1hot_use))\n",
    "            c_1hot_encoded.columns = [col + '_' + '%s'%sub for sub in Onehot[col].categories_[0]]\n",
    "            c_1hot_encoded.index = c_1hot_use.index\n",
    "            for columns in c_1hot_encoded.columns:\n",
    "                c_1hot_na[columns] = np.nan\n",
    "            c_1hot_na = c_1hot_na.drop(col,axis=1)\n",
    "            c_1hot_encoded = pd.concat([c_1hot_encoded,c_1hot_na],axis=0)\n",
    "            df = pd.concat([df,c_1hot_encoded],axis=1)\n",
    "        if c_1hot_use.shape[0] == 0:\n",
    "            columns = [col + '_' + '%s'%sub for sub in Onehot[col].categories_[0]]\n",
    "            for column in columns:\n",
    "                c_1hot_na[column] = np.nan\n",
    "            c_1hot_na = c_1hot_na.drop(col,axis=1)  \n",
    "            df = pd.concat([df,c_1hot_na],axis=1)\n",
    "    return(df)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\softinstall\\Anaconda3\\envs\\tf2\\lib\\site-packages\\ipykernel_launcher.py:25: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "D:\\softinstall\\Anaconda3\\envs\\tf2\\lib\\site-packages\\ipykernel_launcher.py:25: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "D:\\softinstall\\Anaconda3\\envs\\tf2\\lib\\site-packages\\ipykernel_launcher.py:25: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "D:\\softinstall\\Anaconda3\\envs\\tf2\\lib\\site-packages\\ipykernel_launcher.py:19: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "D:\\softinstall\\Anaconda3\\envs\\tf2\\lib\\site-packages\\ipykernel_launcher.py:19: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "D:\\softinstall\\Anaconda3\\envs\\tf2\\lib\\site-packages\\ipykernel_launcher.py:19: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, Onehot, new_columns = OneHotEncoder_fit_transform(X_train,feature_2_onehotencoding)\n",
    "X_test = OneHotEncoder_transform(X_test,feature_2_onehotencoding,Onehot)\n",
    "X_on_test_site = OneHotEncoder_transform(X_on_test_site,feature_2_onehotencoding,Onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drop the original columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(feature_2_onehotencoding,axis=1)\n",
    "X_test = X_test.drop(feature_2_onehotencoding,axis=1)\n",
    "X_on_test_site = X_on_test_site.drop(feature_2_onehotencoding,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impute missing data using KNN, five Ks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. drop features with >50% missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "Miss_count = X_train.count(0)\n",
    "Col_to_drop = Miss_count[Miss_count <= 0.5*X_train.shape[0]].index.tolist()\n",
    "Col_to_drop\n",
    "X_train.drop(Col_to_drop,axis=1,inplace=True)\n",
    "X_test.drop(Col_to_drop,axis=1,inplace=True)\n",
    "X_on_test_site.drop(Col_to_drop,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import KNNImputer\n",
    "class KNNImputer_Ks(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, *Ks):\n",
    "        self.Ks = Ks\n",
    "    def fit(self, X,Ks):\n",
    "        D_imputer = {}        \n",
    "        for k in [3,4,5,6,7]:\n",
    "            imputer = KNNImputer(n_neighbors=k)\n",
    "            D_imputer[k] = imputer.fit(X)              \n",
    "        return D_imputer\n",
    "    def transform(self, X):\n",
    "        Impute_train = {}\n",
    "        for k in [3,4,5,6,7]:\n",
    "            Impute_train[k] = pd.DataFrame(D_imputer[k].transform(X))\n",
    "            Impute_train[k].index = X.index\n",
    "            Impute_train[k].columns = X.columns \n",
    "            if k == 3:\n",
    "                Imputed = Impute_train[k].copy(deep=True)\n",
    "                Imputed.loc[:,:] = 0\n",
    "            Imputed = Imputed.add(Impute_train[k],fill_value=0)\n",
    "        return Imputed/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer_knn = KNNImputer_Ks()\n",
    "D_imputer = imputer_knn.fit(X_train, Ks=\"3,4,5,6,7\")\n",
    "X_train_KNN = imputer_knn.transform(X_train)\n",
    "X_test_KNN = imputer_knn.transform(X_test)\n",
    "X_on_test_site_KNN =  imputer_knn.transform(X_on_test_site)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### round the imputed values for binary features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in new_columns:\n",
    "    X_train_KNN[col] = round(X_train_KNN[col],0)\n",
    "    X_test_KNN[col] = round(X_test_KNN[col],0)\n",
    "    X_on_test_site_KNN[col] = round(X_on_test_site_KNN[col],0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "if y_train.shape[1] == 1:\n",
    "    param_grid = {'max_depth':[3, 5, 10], \\\n",
    "              'max_features': [0.1, 0.5, 'sqrt', 'log2', None], \\\n",
    "              'n_estimators': [10, 100,500,1000]}\n",
    "    Reg_Mol = RandomForestRegressor()\n",
    "    \n",
    "if y_train.shape[1] > 1:\n",
    "    param_grid = {'estimator__max_depth':[3, 5, 10], \\\n",
    "              'estimator__max_features': [0.1, 0.5, 'sqrt', 'log2', None], \\\n",
    "              'estimator__n_estimators': [10, 100,500,1000]}    \n",
    "    Reg_Mol = MultiOutputRegressor(RandomForestRegressor(), n_jobs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "if y_train.shape[1] == 1:\n",
    "    param_grid = {'max_depth':[3, 5, 10], \\\n",
    "              'max_features': [0.1, 0.5, 'sqrt', 'log2', None], \\\n",
    "              'n_estimators': [10, 100,500,1000]}\n",
    "    Reg_Mol = RandomForestRegressor()\n",
    "    \n",
    "if y_train.shape[1] > 1:\n",
    "    param_grid = {'estimator__max_depth':[3], \\\n",
    "              'estimator__max_features': [None], \\\n",
    "              'estimator__n_estimators': [10, 100,500,1000]}    \n",
    "    Reg_Mol = MultiOutputRegressor(RandomForestRegressor(), n_jobs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  31 tasks      | elapsed:   14.4s\n",
      "[Parallel(n_jobs=5)]: Done  40 out of  40 | elapsed:   21.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  31 tasks      | elapsed:   16.9s\n",
      "[Parallel(n_jobs=5)]: Done  40 out of  40 | elapsed:   23.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  31 tasks      | elapsed:   14.6s\n",
      "[Parallel(n_jobs=5)]: Done  40 out of  40 | elapsed:   20.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'max_depth': 3, 'max_features': None, 'n_estimators': 100}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "if y_train.shape[1] == 1:\n",
    "    param_grid = {'max_depth':[3, 5, 10], \\\n",
    "              'max_features': [0.1, 0.5, 'sqrt', 'log2', None], \\\n",
    "              'n_estimators': [10, 100,500,1000]}\n",
    "    Reg_Mol = RandomForestRegressor()\n",
    "    grid_search = GridSearchCV(Reg_Mol, param_grid, cv=cv_num, \\\n",
    "                           scoring='neg_mean_squared_error', verbose=2,n_jobs=5)\n",
    "    grid_search.fit(X_train_KNN, y_train)\n",
    "if y_train.shape[1] > 1:    \n",
    "    gsc = GridSearchCV(\n",
    "                estimator=RandomForestRegressor(),\n",
    "                param_grid={'max_depth':[3, 5, 10], \\\n",
    "                  'max_features': [0.1, 0.5, 'sqrt', 'log2', None], \\\n",
    "                  'n_estimators': [10, 100,500,1000]},\n",
    "                cv=cv_num, scoring='neg_mean_squared_error', verbose=2, n_jobs=5)\n",
    "    grid_search = MultiOutputRegressor(gsc).fit(X_train_KNN, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 3, 'max_features': None, 'n_estimators': 500}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_result.estimators_[2].best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(Reg_Mol, param_grid, cv=cv_num, \\\n",
    "                           scoring='neg_mean_squared_error', verbose=2,n_jobs=5)\n",
    "grid_search.fit(X_train_KNN, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = grid_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. cross-validation and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score\n",
    "parameter2use = grid_search.best_params_\n",
    "\n",
    "if y_train.shape[1] == 1:\n",
    "    Reg = RandomForestRegressor(n_estimators=parameter2use['n_estimators'],\\\n",
    "                            max_depth=parameter2use['max_depth'],\\\n",
    "                            max_features= parameter2use['max_features'],\\\n",
    "                            criterion='mse', random_state=42)    \n",
    "if y_train.shape[1] > 1:\n",
    "    Reg = MultiOutputRegressor(RandomForestRegressor(n_estimators=parameter2use['estimator__n_estimators'],\\\n",
    "                            max_depth=parameter2use['estimator__max_depth'],\\\n",
    "                            max_features= parameter2use['estimator__max_features'],\\\n",
    "                            criterion='mse', random_state=42), n_jobs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mse: Mean squared error regression loss\n",
    "# evs: Explained variance regression score\n",
    "# r2: (coefficient of determination) regression score. \n",
    "    # Best possible score is 1.0 and it can be negative \n",
    "    # (because the model can be arbitrarily worse). \n",
    "    # A constant model that always predicts the expected value of y, \n",
    "    # disregarding the input features, would get a R^2 score of 0.0.\n",
    "# cor: Pearson Correlation Coefficient between true y and predicted y\n",
    "if y_train.shape[1] == 1:\n",
    "    cv_pred = cross_val_predict(estimator=Reg, X=X_train_KNN, y=y_train, cv=cv_num)\n",
    "    cv_mse = mean_squared_error(y_train, cv_pred)\n",
    "    cv_evs = explained_variance_score(y_train, cv_pred)\n",
    "    cv_r2 = r2_score(y_train, cv_pred)\n",
    "    cv_cor = np.corrcoef(np.array(y_train), cv_pred)[0,1]\n",
    "if y_train.shape[1] > 1:\n",
    "    cv_pred = pd.DataFrame(cross_val_predict(estimator=Reg, X=X_train_KNN, y=y_train, cv=cv_num))\n",
    "    cv_pred.columns = y_train.columns\n",
    "    cv_pred.index = y_train.index\n",
    "    cv_mse = mean_squared_error(y_train, cv_pred,multioutput='raw_values')\n",
    "    cv_evs = explained_variance_score(y_train, cv_pred,multioutput='raw_values')\n",
    "    cv_r2 = r2_score(y_train, cv_pred,multioutput='raw_values')\n",
    "    cv_cor = np.corrcoef(np.array(y_train), cv_pred)[0,1]\n",
    "    \n",
    "# need to figure out how to get the multioutput PCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01499697, -0.02130138, -0.02601508, -0.02418396, -0.01632604,\n",
       "       -0.00672137, -0.01345846, -0.01587304, -0.02855328,  0.00227371,\n",
       "       -0.01996841, -0.02209687, -0.00190153, -0.02549373, -0.01205259,\n",
       "       -0.02363729, -0.02879485, -0.0216127 , -0.01878749, -0.01627443,\n",
       "       -0.01860381, -0.01431295, -0.01635258, -0.00749156, -0.01046488,\n",
       "       -0.018761  , -0.02371008, -0.01564974, -0.00690794, -0.01592981,\n",
       "        0.00831275, -0.0267923 , -0.01313112, -0.02191073, -0.0210646 ,\n",
       "       -0.01413702,  0.0021471 , -0.0128265 , -0.00231004, -0.03602125,\n",
       "       -0.02395407, -0.01610284])"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_evs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reg.fit(X_train_KNN,y_train)\n",
    "pred_train = Reg.predict(X_train_KNN)\n",
    "if y_train.shape[1] == 1:\n",
    "    train_mse = mean_squared_error(y_train, pred_train)\n",
    "    train_evs = explained_variance_score(y_train, pred_train)\n",
    "    train_r2 = r2_score(y_train, pred_train)\n",
    "    train_cor = np.corrcoef(np.array(y_train), pred_train)[0,1]\n",
    "if y_train.shape[1] > 1:\n",
    "    pred_train = pd.DataFrame(pred_train)\n",
    "    pred_train.columns = y_train.columns\n",
    "    pred_train.index = y_train.index\n",
    "    train_mse = mean_squared_error(y_train, pred_train,multioutput='raw_values')\n",
    "    train_evs = explained_variance_score(y_train, pred_train,multioutput='raw_values')\n",
    "    train_r2 = r2_score(y_train, pred_train,multioutput='raw_values')\n",
    "    train_cor = np.corrcoef(np.array(y_train), pred_train)[0,1]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = Reg.predict(X_test_KNN)\n",
    "if y_train.shape[1] == 1:\n",
    "    test_mse = mean_squared_error(y_test, pred_test)\n",
    "    test_evs = explained_variance_score(y_test, pred_test)\n",
    "    test_r2 = r2_score(y_test, pred_test)\n",
    "    test_cor = np.corrcoef(np.array(y_test), pred_test)\n",
    "if y_train.shape[1] > 1:\n",
    "    pred_test = pd.DataFrame(pred_test)\n",
    "    pred_test.columns = y_test.columns\n",
    "    pred_test.index = y_test.index\n",
    "    test_mse = mean_squared_error(y_test, pred_test,multioutput='raw_values')\n",
    "    test_evs = explained_variance_score(y_test, pred_test,multioutput='raw_values')\n",
    "    test_r2 = r2_score(y_test, pred_test,multioutput='raw_values')\n",
    "    test_cor = np.corrcoef(np.array(y_test), pred_test)[0,1]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_on_test_site = Reg.predict(X_on_test_site_KNN)\n",
    "if y_train.shape[1] == 1:\n",
    "    test_site_mse = mean_squared_error(y_on_test_site, pred_on_test_site)\n",
    "    test_site_evs = explained_variance_score(y_on_test_site, pred_on_test_site)\n",
    "    test_site_r2 = r2_score(y_on_test_site, pred_on_test_site)\n",
    "    test_site_cor = np.corrcoef(np.array(y_on_test_site), pred_on_test_site)[0,1]\n",
    "if y_train.shape[1] > 1:\n",
    "    pred_on_test_site = pd.DataFrame(pred_on_test_site)\n",
    "    pred_on_test_site.columns = y_on_test_site.columns\n",
    "    pred_on_test_site.index = y_on_test_site.index\n",
    "    test_site_mse = mean_squared_error(y_on_test_site, pred_on_test_site,multioutput='raw_values')\n",
    "    test_site_evs = explained_variance_score(y_on_test_site, pred_on_test_site,multioutput='raw_values')\n",
    "    test_site_r2 = r2_score(y_on_test_site, pred_on_test_site,multioutput='raw_values')\n",
    "    test_site_cor = np.corrcoef(np.array(y_on_test_site), pred_on_test_site)[0,1]            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the fitted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(Reg, open(save_model_name, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = open(save_result_name,'w')\n",
    "out.write('The model is built using data from %s, and applied to %s and %s.\\n\\n'%(site,site,site_not_used))\n",
    "out.write('There are %s training instances.\\n'%X_train_KNN.shape[0])\n",
    "out.write('There are %s test instances.\\n'%X_test_KNN.shape[0])\n",
    "out.write('There are %s instances in the other site.\\n\\n'%X_on_test_site_KNN.shape[0])\n",
    "out.write('The model is built using RandomForestRegressor, with:\\n')\n",
    "if y_train.shape[1] == 1:\n",
    "    out.write('\\tn_estimators: %s\\n'%parameter2use['n_estimators'])\n",
    "    out.write('\\tmax_depth: %s\\n'%parameter2use['max_depth'])\n",
    "    out.write('\\tmax_features: %s\\n\\n'%parameter2use['max_features'])\n",
    "if y_train.shape[1] > 1:\n",
    "    out.write('\\tn_estimators: %s\\n'%parameter2use['estimator__n_estimators'])\n",
    "    out.write('\\tmax_depth: %s\\n'%parameter2use['estimator__max_depth'])\n",
    "    out.write('\\tmax_features: %s\\n\\n'%parameter2use['estimator__max_features'])    \n",
    "out.write('There are %s feature used\\n\\n'%X_train_KNN.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.write('Prediction\\tmse\\tevs\\tr2\\tPCC\\n')\n",
    "out.write('CV\\t%s\\t%s\\t%s\\t%s\\n'%(cv_mse,cv_evs,cv_r2,cv_cor))\n",
    "out.write('Train\\t%s\\t%s\\t%s\\t%s\\n'%(train_mse,train_evs,train_r2,train_cor))\n",
    "out.write('Test\\t%s\\t%s\\t%s\\t%s\\n'%(test_mse,test_evs,test_r2,test_cor))\n",
    "out.write('Other_site\\t%s\\t%s\\t%s\\t%s\\n\\n'%(test_site_mse,test_site_evs,test_site_r2,test_site_cor))\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = pd.DataFrame({'Feature':X_train_KNN.columns, 'Importance':Reg.feature_importances_})\n",
    "imp_sorted = imp.sort_values(by='Importance', ascending=False)\n",
    "imp_sorted.to_csv(save_result_name.split('.txt')[0] + '_imp.txt', index=False, header=True,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the importance direction in the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
