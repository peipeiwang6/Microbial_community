{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the gene-pathway annotation\n",
    "df = pd.read_csv('Sly_pathway_annotation_20180827_with_expression_5_members.txt',header=None,index_col=None,sep='\\t')\n",
    "# gene-pathway annotation for only non-overlapping genes in 85 pathways\n",
    "df_uni = pd.read_csv('Sly_pathway_annotation_20190117_with_expression_5_members_nonoverlapping.txt',header=None,index_col=None,sep='\\t')\n",
    "df.columns = ['Pathway','Gene']\n",
    "pathway = df_uni[0].unique()\n",
    "# to get back genes with multiple pathway annotation in only those 85 pathways\n",
    "df_85 = df[df['Pathway'].isin(pathway)]\n",
    "gene = df_85['Gene'].unique()\n",
    "df_85.index = df_85['Gene']\n",
    "df_85 = df_85.drop(['Gene'],axis=1)\n",
    "df_85.columns = ['Pathway']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make the multiple labels, using OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_encoder = OneHotEncoder() # or cat_encoder = OneHotEncoder(sparse=False)\n",
    "df_85_1hot = cat_encoder.fit_transform(df_85)\n",
    "labels = pd.DataFrame(df_85_1hot.toarray()) # or labels = pd.DataFrame(df_85_1hot)\n",
    "labels.columns = cat_encoder.categories_[0]\n",
    "labels.index = df_85.index\n",
    "# group rows with same index ID, add up the 1s\n",
    "gp = labels.groupby(level=0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parse the expression matrix, here FCs in all combination were used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = pd.read_csv('Results_Fold_changes_all_combination_Sly_20180124.txt',header=0,index_col=None,sep='\\t')\n",
    "exp_85 = exp[exp['gene'].isin(gene)]\n",
    "exp_85.index = exp_85['gene']\n",
    "exp_85 = exp_85.drop('gene',axis=1)\n",
    "res = pd.concat([gp,exp_85],axis=1)\n",
    "# save the multilable matrix\n",
    "res.to_csv('Multilabel_85_pathways_matrix_all_FC.txt',index=True, header=True,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Multilabel_85_pathways_matrix_all_FPKM.txt',header=0,index_col=0,sep='\\t')\n",
    "X = df.iloc[:,85:]\n",
    "y = df.iloc[:,0:85]\n",
    "# count no of pathways each gene was annotated\n",
    "freq = y.astype(bool).sum(axis=1)\n",
    "# get genes with multiple pathway annotation\n",
    "freq2 = freq[freq>1]\n",
    "# get the corresponding labels\n",
    "y2 = y.loc[y.index.isin(freq2.index),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count no of genes annotated in each pathway\n",
    "freq3 = y2.astype(bool).sum(axis=0)\n",
    "sorted(freq3,reverse=True)\n",
    "# get pathways with more than 10 genes\n",
    "freq3[freq3 > 10]\n",
    "y3 = y.loc[:,['PWY-321','PWY-361','PWY-5690','PWY-6443','PWY-6733']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PWY-321</th>\n",
       "      <th>PWY-361</th>\n",
       "      <th>PWY-5690</th>\n",
       "      <th>PWY-6443</th>\n",
       "      <th>PWY-6733</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NP_001234001.1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NP_001234005.1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NP_001234277.1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NP_001234574.1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NP_001234767.1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                PWY-321  PWY-361  PWY-5690  PWY-6443  PWY-6733\n",
       "NP_001234001.1      0.0      0.0       1.0       0.0       0.0\n",
       "NP_001234005.1      0.0      0.0       1.0       0.0       0.0\n",
       "NP_001234277.1      0.0      0.0       1.0       0.0       0.0\n",
       "NP_001234574.1      0.0      0.0       1.0       0.0       0.0\n",
       "NP_001234767.1      0.0      0.0       0.0       1.0       0.0"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for the remaining dataset, get rid of genes which are not annotated in the 5 pathways\n",
    "freq4 = y3.astype(bool).sum(axis=1)\n",
    "freq5 = freq4[freq4 > 0]\n",
    "y4 = y3.loc[y3.index.isin(freq5.index),:]\n",
    "y4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.loc[X.index.isin(y4.index),:]\n",
    "y = y4\n",
    "df = pd.concat([y,X],axis=1)\n",
    "df.to_csv('Multilabel_5_pathways_matrix_all_FPKM.txt',index=True, header=True,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data to training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'type'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-e0ddd4912dd3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\softinstall\\Anaconda3\\envs\\tf2\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5177\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5178\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5179\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5181\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'type'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Multilabel_5_pathways_matrix_all_FPKM.txt',header=0,index_col=0,sep='\\t')\n",
    "X = df.iloc[:,5:]\n",
    "y = df.iloc[:,0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### StratifiedKFold can't be used in multi-label cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import StratifiedKFold\n",
    "#skfolds = StratifiedKFold(n_splits=5, random_state=42,shuffle=True)\n",
    "#train_index, test_index = skfolds.split(X, y)\n",
    "# ValueError: Supported target types are: ('binary', 'multiclass'). Got 'multilabel-indicator' instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### !!Note that IterativeStratification can't freeze the random state, therefore every run would lead to different split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skmultilearn\n",
    "from skmultilearn.model_selection import IterativeStratification\n",
    "import numpy as np\n",
    "#k_fold = IterativeStratification(n_splits=5, order=1)\n",
    "#Split = pd.DataFrame(k_fold.split(X, y))\n",
    "#X_train = X.iloc[Split.iloc[0,0],:]\n",
    "#y_train = y.iloc[Split.iloc[0,0],:]\n",
    "#X_test = X.iloc[Split.iloc[0,1],:]\n",
    "#y_test = y.iloc[Split.iloc[0,1],:]\n",
    "#ID_train = y_train.index.tolist()\n",
    "#pd.DataFrame(ID_train).to_csv('Multilable_training_ID.txt',index=False, header=False,sep=\"\\t\")\n",
    "#ID_test = y_test.index.tolist()\n",
    "#pd.DataFrame(ID_test).to_csv('Multilable_test_ID.txt',index=False, header=False,sep=\"\\t\")\n",
    "# count non zeros for each column (pathway annotation)\n",
    "#y_train.astype(bool).sum(axis=0)\n",
    "#y_test.astype(bool).sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reload the saved training and test gene IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 168 entries, NP_001234001.1 to XP_010326994.1\n",
      "Columns: 377 entries, PWY-321 to 2017_yield.s2_mutant_TM\n",
      "dtypes: float64(377)\n",
      "memory usage: 496.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Multilabel_5_pathways_matrix_all_FPKM.txt',header=0,index_col=0,sep='\\t')\n",
    "train_id = pd.read_csv('Multilable_training_ID.txt',header=None,index_col=None,sep='\\t')\n",
    "test_id = pd.read_csv('Multilable_test_ID.txt',header=None,index_col=None,sep='\\t')\n",
    "X_train = df.loc[df.index.isin(train_id[0]),:].iloc[:,5:]\n",
    "y_train = df.iloc[df.index.isin(train_id[0]),:].iloc[:,0:5]\n",
    "X_test = df.loc[df.index.isin(test_id[0]),:].iloc[:,5:]\n",
    "y_test = df.iloc[df.index.isin(test_id[0]),:].iloc[:,0:5]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impute the missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import KNNImputer\n",
    "class KNNImputer_Ks(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, *Ks):\n",
    "        self.Ks = Ks\n",
    "    def fit(self, X,Ks):\n",
    "        D_imputer = {}        \n",
    "        for k in [3,4,5,6,7]:\n",
    "            imputer = KNNImputer(n_neighbors=k)\n",
    "            D_imputer[k] = imputer.fit(X)              \n",
    "        return D_imputer\n",
    "    def transform(self, X):\n",
    "        Impute_train = {}\n",
    "        for k in [3,4,5,6,7]:\n",
    "            Impute_train[k] = pd.DataFrame(D_imputer[k].transform(X))\n",
    "            Impute_train[k].index = X.index\n",
    "            Impute_train[k].columns = X.columns \n",
    "            if k == 3:\n",
    "                Imputed = Impute_train[k].copy(deep=True)\n",
    "                Imputed.loc[:,:] = 0\n",
    "            Imputed = Imputed.add(Impute_train[k],fill_value=0)\n",
    "        return Imputed/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer_knn = KNNImputer_Ks()\n",
    "D_imputer = imputer_knn.fit(X_train, Ks=\"3,4,5,6,7\")\n",
    "X_train_KNN = imputer_knn.transform(X_train)\n",
    "X_test_KNN = imputer_knn.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_KNN = scaler.fit_transform(X_train_KNN)\n",
    "X_test_KNN = scaler.transform(X_test_KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "knn_clf = KNeighborsClassifier()\n",
    "param_grid = {'n_neighbors':[3,4,5,10,15,20,25], \\\n",
    "              'weights': ['uniform', 'distance'], \\\n",
    "              'metric': ['euclidean','manhattan','minkowski']}\n",
    "# add randomize search\n",
    "# cut down the feature size to 30\n",
    "# top 5 pathways\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'random_state'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-e7f140023c7a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mknn_clf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'f1_weighted'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_KNN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'random_state'"
     ]
    }
   ],
   "source": [
    "gs = GridSearchCV(knn_clf, param_grid, cv=5, scoring='f1_weighted', verbose=2, n_jobs=5)\n",
    "gs.fit(X_train_KNN,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.46151055, 0.20147293, 0.35882353, 0.40320513, 0.27708333])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "parameter2use = gs.best_params_\n",
    "KNC_clf = KNeighborsClassifier(metric=parameter2use['metric'],\\\n",
    "                            n_neighbors=parameter2use['n_neighbors'],\\\n",
    "                            weights= parameter2use['weights'])\n",
    "cv_pred = cross_val_predict(estimator=KNC_clf, X=X_train_KNN, y=y_train, cv=5)\n",
    "cv_score = cross_val_score(estimator=KNC_clf, X=X_train_KNN, y=y_train, \\\n",
    "                           cv=5,scoring='f1_weighted')\n",
    "# using three F1 values: macro, average, weighted\n",
    "cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    23\n",
       "1    21\n",
       "2    22\n",
       "3    33\n",
       "4    26\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(cv_pred).astype(bool).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.24489796, 0.30769231, 0.3902439 , 0.39506173, 0.36363636])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "F1_weighted_cv = f1_score(y_train,cv_pred,average = 'weighted')\n",
    "f1_score(y_train,cv_pred,average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4915069057926201"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNC_clf.fit(X_train_KNN,y_train)\n",
    "test_pred = KNC_clf.predict(X_test_KNN)\n",
    "F1_weighted_test = f1_score(y_test,test_pred,average = 'weighted')\n",
    "F1_weighted_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48718614718614717"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1_macro_test = f1_score(y_test,test_pred,average = 'macro')\n",
    "F1_macro_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.42857143, 0.42857143, 0.5       , 0.54545455, 0.53333333])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1_test = f1_score(y_test,test_pred,average = None)\n",
    "F1_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4444444444444444"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate metrics for each instance, and find their average (only meaningful for multilabel classification where this differs from accuracy_score).\n",
    "F1_sample_test = f1_score(y_test,test_pred,average = 'samples')\n",
    "F1_sample_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f1 score for each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.42857143, 0.42857143, 0.5       , 0.54545455, 0.53333333])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test,test_pred,average=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "RF_clf = RandomForestClassifier()\n",
    "param_grid = {'max_depth':[3, 5, 10], \\\n",
    "              'max_features': [0.1, 0.5, 'sqrt', 'log2', None], \\\n",
    "              'n_estimators': [2,3,4,5,10,20, 30, 40, 50, 100,200,300,400,500,1000]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 225 candidates, totalling 1125 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  31 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=5)]: Done 197 tasks      | elapsed:   28.5s\n",
      "[Parallel(n_jobs=5)]: Done 430 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=5)]: Done 713 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=5)]: Done 1098 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=5)]: Done 1125 out of 1125 | elapsed:  4.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                              class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features='auto',\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              max_samples=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              n_estimators=100, n_jobs=None,\n",
       "                                              oob_score=False,\n",
       "                                              random_state=None, verbose=0,\n",
       "                                              warm_start=False),\n",
       "             iid='deprecated', n_jobs=5,\n",
       "             param_grid={'max_depth': [3, 5, 10],\n",
       "                         'max_features': [0.1, 0.5, 'sqrt', 'log2', None],\n",
       "                         'n_estimators': [2, 3, 4, 5, 10, 20, 30, 40, 50, 100,\n",
       "                                          200, 300, 400, 500, 1000]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='f1_weighted', verbose=2)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs = GridSearchCV(RF_clf, param_grid, cv=5, scoring='f1_weighted', verbose=2,\\\n",
    "                 n_jobs=5)\n",
    "gs.fit(X_train_KNN,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 5, 'max_features': None, 'n_estimators': 2}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.26455259, 0.30519481, 0.18823529, 0.28938439, 0.10238095])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "parameter2use = gs.best_params_\n",
    "#parameter2use = {'max_depth':3,'max_features': 0.1,'n_estimators':100}\n",
    "RF_clf = RandomForestClassifier(max_depth=parameter2use['max_depth'],\\\n",
    "                            max_features=parameter2use['max_features'],\\\n",
    "                            n_estimators= parameter2use['n_estimators'],\\\n",
    "                            random_state=42)\n",
    "cv_score = cross_val_score(estimator=RF_clf, X=X_train_KNN, y=y_train, \\\n",
    "                           cv=5,scoring='f1_weighted')\n",
    "cv_pred = cross_val_predict(estimator=RF_clf, X=X_train_KNN, y=y_train, \\\n",
    "                            cv=5)\n",
    "cv_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the output and the tree structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "#RF_clf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    13\n",
       "1    37\n",
       "2    10\n",
       "3    30\n",
       "4    14\n",
       "dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(cv_pred).astype(bool).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.20512821, 0.37037037, 0.4137931 , 0.1025641 , 0.13953488])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "F1_weighted_cv = f1_score(y_train,cv_pred,average = 'weighted')\n",
    "f1_score(y_train,cv_pred,average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34397031539888684"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_clf.fit(X_train_KNN,y_train)\n",
    "test_pred = RF_clf.predict(X_test_KNN)\n",
    "F1_weighted_test = f1_score(y_test,test_pred,average = 'weighted')\n",
    "F1_weighted_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3800999000999001"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1_macro_test = f1_score(y_test,test_pred,average = 'macro')\n",
    "F1_macro_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-layer Perceptron classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "MLP_clf = MLPClassifier()\n",
    "param_grid = {'solver': ['lbfgs'], \\\n",
    "              'max_iter': [100, 200], \\\n",
    "              'alpha': 10.0 ** -np.arange(1, 2), \\\n",
    "              'hidden_layer_sizes': [10, 20, 50, 100, 200], \\\n",
    "              'activation': ['logistic'], \\\n",
    "              'learning_rate': [ 'adaptive']\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  31 tasks      | elapsed:    6.9s\n",
      "[Parallel(n_jobs=5)]: Done  50 out of  50 | elapsed:   22.8s finished\n",
      "D:\\softinstall\\Anaconda3\\envs\\tf2\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=MLPClassifier(activation='relu', alpha=0.0001,\n",
       "                                     batch_size='auto', beta_1=0.9,\n",
       "                                     beta_2=0.999, early_stopping=False,\n",
       "                                     epsilon=1e-08, hidden_layer_sizes=(100,),\n",
       "                                     learning_rate='constant',\n",
       "                                     learning_rate_init=0.001, max_fun=15000,\n",
       "                                     max_iter=200, momentum=0.9,\n",
       "                                     n_iter_no_change=10,\n",
       "                                     nesterovs_momentum=True, power_t=0.5,\n",
       "                                     random_state...\n",
       "                                     solver='adam', tol=0.0001,\n",
       "                                     validation_fraction=0.1, verbose=False,\n",
       "                                     warm_start=False),\n",
       "             iid='deprecated', n_jobs=5,\n",
       "             param_grid={'activation': ['logistic'], 'alpha': array([0.1]),\n",
       "                         'hidden_layer_sizes': [10, 20, 50, 100, 200],\n",
       "                         'learning_rate': ['adaptive'], 'max_iter': [100, 200],\n",
       "                         'solver': ['lbfgs']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='f1_weighted', verbose=2)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_MLP = GridSearchCV(MLP_clf, param_grid, cv=5, scoring='f1_weighted', verbose=2,\\\n",
    "                 n_jobs=5)\n",
    "gs_MLP.fit(X_train_KNN,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'logistic',\n",
       " 'alpha': 0.1,\n",
       " 'hidden_layer_sizes': 20,\n",
       " 'learning_rate': 'adaptive',\n",
       " 'max_iter': 100,\n",
       " 'solver': 'lbfgs'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_MLP.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\softinstall\\Anaconda3\\envs\\tf2\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "D:\\softinstall\\Anaconda3\\envs\\tf2\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "D:\\softinstall\\Anaconda3\\envs\\tf2\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "D:\\softinstall\\Anaconda3\\envs\\tf2\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "D:\\softinstall\\Anaconda3\\envs\\tf2\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "D:\\softinstall\\Anaconda3\\envs\\tf2\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "D:\\softinstall\\Anaconda3\\envs\\tf2\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "D:\\softinstall\\Anaconda3\\envs\\tf2\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "D:\\softinstall\\Anaconda3\\envs\\tf2\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "D:\\softinstall\\Anaconda3\\envs\\tf2\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.52630037, 0.40769588, 0.44423224, 0.34971697, 0.41284271])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "parameter2use = gs_MLP.best_params_\n",
    "MLP_clf = MLPClassifier(solver=parameter2use['solver'], \\\n",
    "                        max_iter=parameter2use['max_iter'], \\\n",
    "                        alpha=parameter2use['alpha'], \\\n",
    "                        hidden_layer_sizes=parameter2use['hidden_layer_sizes'], \\\n",
    "                        activation=parameter2use['activation'], \\\n",
    "                        learning_rate=parameter2use['learning_rate'])\n",
    "cv_score = cross_val_score(estimator=MLP_clf, X=X_train_KNN, y=y_train, \\\n",
    "                           cv=5,scoring='f1_weighted')\n",
    "cv_pred = cross_val_predict(estimator=MLP_clf, X=X_train_KNN, y=y_train, \\\n",
    "                            cv=5)\n",
    "cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    22\n",
       "1    37\n",
       "2    20\n",
       "3    37\n",
       "4    30\n",
       "dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(cv_pred).astype(bool).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.29166667, 0.51851852, 0.61538462, 0.4       , 0.33898305])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "F1_weighted_cv = f1_score(y_train,cv_pred,average = 'weighted')\n",
    "f1_score(y_train,cv_pred,average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\softinstall\\Anaconda3\\envs\\tf2\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4177846577227382"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP_clf.fit(X_train_KNN,y_train)\n",
    "test_pred = MLP_clf.predict(X_test_KNN)\n",
    "F1_weighted_test = f1_score(y_test,test_pred,average = 'weighted')\n",
    "F1_weighted_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42672858617131054"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1_macro_test = f1_score(y_test,test_pred,average = 'macro')\n",
    "F1_macro_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
